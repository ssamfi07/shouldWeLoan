hclust_model <- hierarchical_clustering(df_train_processed)
# Partitional Clustering
cluster_cut <- partitional_clustering(pca_train$x[, 1:2], pcs_only)
partitional_clustering <- function(df, pcs_only) {
# get the hierarchical model
hclust_model <- hierarchical_clustering(df)
#graph with the within cluster sum of squares to determine the ideal number of clusters
wcss <- vector()
for (i in 1:10) {
kmeans_model <- kmeans(pcs_only , centers = i, nstart = 10)
wcss[i] <- kmeans_model$tot.withinss
}
plot(1:10, wcss, type = "b", xlab = "Number of Clusters", ylab = "WCSS")
diff_wcss <- c(0, diff(wcss))
#we apply the elbow method, and the ideal number of clusters is 3 or 4
#we will apply partitional clustering with 3 and 4 clusters and determine which one is better
num_clusters <- 3
cluster_cut <- cutree(hclust_model, k = num_clusters)
#adds a new column to the data frame with the clusters id
pcs_only$Cluster <- as.factor(cluster_cut)
#visualize the clusters
ggplot(pcs_only, aes(x = PC1, y = PC2, color = Cluster)) +
geom_point() +
labs(title = "Cluster Analysis", x = "PC1", y = "PC2")
#silhouette coefficient to measure how good the partitional clustering is
sf <- silhouette(cluster_cut, dist(pcs_only))
sf_avg <- mean(sf[, 3])
print(sf_avg)
# 0.30 is not a very good result
# update -- obtained 0.446469
# sad update also obtained 0.27
#k-means (to numeric columns)
numeric_cols <- sapply(pcs_only, is.numeric)
pcs_only_numeric <- pcs_only[, numeric_cols]
kmeans_model <- kmeans(pcs_only, centers = num_clusters, nstart = 25)
#visualize the clusters from k-means
fviz_cluster(kmeans_model, data = pcs_only_numeric, geom = "point",
ellipse.type = "t", ggtheme = theme_classic(),
ellipse.alpha = 0.5, palette = "jco",
main = "K-means Clustering Results",
xlab = "PC1", ylab = "PC2")
#3 clusters seems to be the ideal number of clusters
# silhouette coefficient to measure how good the k-means is
sf_kmeans <- silhouette(kmeans_model$cluster, dist(pcs_only_numeric))
sf_kmeans_avg <- mean(sf_kmeans[,3])
print(sf_kmeans_avg)
# 0.36 is not a very good result
# update neither is 0.32
return(cluster_cut)
}
cluster_cut <- partitional_clustering(pca_train$x[, 1:2], pcs_only)
# read the csv files
read_and_preprocess <- function(file_path) {
df <- read_csv(file_path)
print(colnames(df))
# additional preprocessing steps here
# we don't need "trans_id" or "account_id" -> remove
df <- df[, !colnames(df) %in% c("trans_id")]
df <- df[, !colnames(df) %in% c("account_id")]
# convert data_diff to numeric
df$date_diff <- as.numeric(df$date_diff)
# convert our status target variable to binary -- 1 for paid 0 for unpaid
df$status_binary <- ifelse(df$status == 1, 1, 0)
df <- df[, !colnames(df) %in% c("status")]
# check for missing values
colSums(is.na(df))
# we have missing values in the columns "date_diff"
# so we need to change the N/A values to "0"
df$date_diff[is.na(df$date_diff)] <- 0
# check for redundant or irrelevant values (or columns)
# check for columns with 0 variance
numeric_df <- df[sapply(df, is.numeric)]
zero_variance_cols <- colnames(numeric_df)[apply(numeric_df, 2, function(x) length(unique(x))) == 1]
print(zero_variance_cols)
# no column has 0 variance
return(df)
}
#DATA TRANSFORMATION
# function to perform data normalization and label encoding
normalize_and_encode <- function(df) {
#normalize every numeric column (with z-score standardization), except for the target variable "status_binary"
numeric_cols <- sapply(df, is.numeric)
numeric_cols["status_binary"] <- FALSE
numeric_cols["loan_id"] <- FALSE
df[numeric_cols] <- scale(df[numeric_cols])
#check for categorical columns
categorical_cols <- names(df)[sapply(df, function(x) is.factor(x) || is.character(x))]
print(categorical_cols)
#only the column "type" is categorical
#so we are going to label encode it
df <- df %>% mutate(type = recode(type, "credit" = 1, "withdrawal" = 2))
#(Pearson) correlation in relation to "status" (our target variable)
correlations <- sapply(df, function(col) cor(col, df$status_binary))
print(correlations)
# the columns that have almost no correlation with "status" will be dropped
# only "date_diff" and "amount_trans" will be dropped -- not sure about this step
# df <- df[, !colnames(df) %in% c("amount_trans", "date_diff")]
return(df)
}
# Read and preprocess the training dataset
df_train <- read_and_preprocess("database.csv")
# Apply normalization and label encoding
df_train_processed <- normalize_and_encode(df_train)
function to perform data normalization and label encoding
# function to perform data normalization and label encoding
normalize_and_encode <- function(df) {
#normalize every numeric column (with z-score standardization), except for the target variable "status_binary"
numeric_cols <- sapply(df, is.numeric)
numeric_cols["status_binary"] <- FALSE
numeric_cols["loan_id"] <- FALSE
df[numeric_cols] <- scale(df[numeric_cols])
#check for categorical columns
categorical_cols <- names(df)[sapply(df, function(x) is.factor(x) || is.character(x))]
print(categorical_cols)
#only the column "type" is categorical
#so we are going to label encode it
df <- df %>% mutate(type = recode(type, "credit" = 1, "withdrawal" = 2))
#(Pearson) correlation in relation to "status" (our target variable)
correlations <- sapply(df, function(col) cor(col, df$status_binary))
print(correlations)
# the columns that have almost no correlation with "status" will be dropped
# only "date_diff" and "amount_trans" will be dropped -- not sure about this step
# df <- df[, !colnames(df) %in% c("amount_trans", "date_diff")]
return(df)
}
# Read and preprocess the training dataset
df_train <- read_and_preprocess("database.csv")
# Apply normalization and label encoding
df_train_processed <- normalize_and_encode(df_train)
print(n=200, df_train_processed)
# Perform PCA
pca_result <- perform_pca(df_train_processed)
pca_train <- pca_result$pca
pcs_only <- pca_result$pcs_only
# Partitional Clustering
cluster_cut <- partitional_clustering(pca_train$x[, 1:2], pcs_only)
print(n=200, df_train_processed)
install.packages("keras")
library(keras)
# Read and preprocess the training dataset
df_train <- read_and_preprocess("database.csv")
# Preprocess the data
df_processed <- df_train %>%
group_by(loan_id) %>%
arrange(date_diff) %>%
ungroup() %>%
mutate(across(c(amount_loan, duration, payments, type, amount_trans, balance, date_diff),
~scale(.),
.names = "{.col}_scaled"))
# Apply normalization and label encoding
df_train_processed <- normalize_and_encode(df_train)
# Define the model
model <- keras_model_sequential() %>%
layer_lstm(units = 50, input_shape = c(NULL, ncol(df_train_processed) - 2)) %>%
layer_dense(units = 1, activation = "sigmoid")
install_tensorflow()
# Define the model
model <- keras_model_sequential() %>%
layer_lstm(units = 50, input_shape = c(NULL, ncol(df_train_processed) - 2)) %>%
layer_dense(units = 1, activation = "sigmoid")
# Compile the model
model %>% compile(
loss = "binary_crossentropy",
optimizer = optimizer_adam(),
metrics = c("accuracy")
)
# Define the model
model <- keras_model_sequential() %>%
layer_lstm(units = 50, input_shape = c(NULL, ncol(df_train_processed) - 2)) %>%
layer_dense(units = 1, activation = "sigmoid")
# Preprocess the data by aggregating features for each loan_id
df_aggregated <- df_train %>%
group_by(loan_id) %>%
summarise(
mean_amount_loan = mean(amount_loan),
mean_duration = mean(duration),
mean_payments = mean(payments),
mean_type = mean(type),
mean_amount_trans = mean(amount_trans),
mean_balance = mean(balance),
mean_date_diff = mean(date_diff),
status_binary = first(status_binary)  # Assuming it's constant for each loan_id
) %>%
ungroup()
# Preprocess the data by aggregating features for each loan_id
df_aggregated <- df_train %>%
group_by(loan_id) %>%
summarise(
mean_amount_loan = mean(amount_loan),
mean_duration = mean(duration),
mean_payments = mean(payments),
mean_type = mean(type),
mean_amount_trans = mean(amount_trans),
mean_balance = mean(balance),
mean_date_diff = mean(date_diff),
status_binary = first(status_binary)  # Assuming it's constant for each loan_id
) %>%
ungroup()
# Preprocess the data by aggregating features for each loan_id
df_aggregated <- df_train %>%
group_by(loan_id) %>%
summarise(
mean_amount_loan = mean(amount_loan),
mean_duration = mean(duration),
mean_payments = mean(payments),
mean_amount_trans = mean(amount_trans),
mean_balance = mean(balance),
mean_date_diff = mean(date_diff),
status_binary = first(status_binary)  # Assuming it's constant for each loan_id
) %>%
ungroup()
# Split the data into training and testing sets
set.seed(42)
train_indices <- sample(1:nrow(df_aggregated), 0.8 * nrow(df_aggregated))
train_data <- df_aggregated[train_indices, ]
test_data <- df_aggregated[-train_indices, ]
# Train a random forest model
rf_model <- randomForest(status_binary ~ ., data = train_data)
# Train a random forest model for classification
rf_model <- randomForest(status_binary ~ ., data = train_data)
# Train a random forest model for classification
rf_model <- randomForest(status_binary ~ ., data = train_data)
# Train a random forest model for classification
rf_model <- randomForest(status_binary ~ ., data = train_data)
# Train a random forest model for classification
rf_model <- randomForest(status_binary ~ ., data = train_data)
# Train a random forest model for classification
rf_model <- randomForest(status_binary ~ ., data = train_data)
# Train a random forest model for classification
rf_model <- randomForest(status_binary ~ ., data = train_data)
# Train a random forest model for classification
rf_model <- randomForest(status_binary ~ ., data = train_data)
# Train a random forest model for classification
rf_model <- randomForest(status_binary ~ ., data = train_data)
# Convert status_binary to a factor with more than two levels
train_data$status_binary <- as.factor(train_data$status_binary)
# Train a random forest model for classification
rf_model <- randomForest(status_binary ~ ., data = train_data)
# Make predictions on the test set
# 'predict' with 'type = "response"' returns class probabilities for each class
predictions <- predict(rf_model, newdata = test_data, type = "response")
# Assuming you have a data frame named 'result_df' with columns 'Id' and 'Probability'
result_df <- data.frame(Id = test_data$loan_id, Probability = predictions[, "0"])
# Preprocess the data by aggregating features for each loan_id
df_aggregated <- df_train %>%
group_by(loan_id) %>%
summarise(
mean_amount_loan = mean(amount_loan),
mean_duration = mean(duration),
mean_payments = mean(payments),
mean_amount_trans = mean(amount_trans),
mean_balance = mean(balance),
mean_date_diff = mean(date_diff),
status_binary = first(status_binary)  # Assuming it's constant for each loan_id
) %>%
ungroup()
# Split the data into training and testing sets
set.seed(42)
train_indices <- sample(1:nrow(df_aggregated), 0.8 * nrow(df_aggregated))
train_data <- df_aggregated[train_indices, ]
test_data <- df_aggregated[-train_indices, ]
# Convert status_binary to a factor with more than two levels
train_data$status_binary <- as.factor(train_data$status_binary)
# Train a random forest model for classification
rf_model <- randomForest(status_binary ~ ., data = train_data)
# Make predictions on the test set
# 'predict' with 'type = "response"' returns class probabilities for each class
predictions <- predict(rf_model, newdata = test_data, type = "response")
# Assuming you have a data frame named 'result_df' with columns 'Id' and 'Probability'
result_df <- data.frame(Id = test_data$loan_id, Probability = predictions[, "0"])
# Preprocess the data by aggregating features for each loan_id
df_aggregated <- df_train %>%
group_by(loan_id) %>%
summarise(
mean_amount_loan = mean(amount_loan),
mean_duration = mean(duration),
mean_payments = mean(payments),
mean_amount_trans = mean(amount_trans),
mean_balance = mean(balance),
mean_date_diff = mean(date_diff),
status_binary = first(status_binary)  # Assuming it's constant for each loan_id
) %>%
ungroup()
# Split the data into training and testing sets
set.seed(42)
train_indices <- sample(1:nrow(df_aggregated), 0.8 * nrow(df_aggregated))
train_data <- df_aggregated[train_indices, ]
test_data <- df_aggregated[-train_indices, ]
# Convert status_binary to a factor with more than two levels
train_data$status_binary <- as.factor(train_data$status_binary)
# Train a random forest model for classification
rf_model <- randomForest(status_binary ~ ., data = train_data)
# Make predictions on the test set
# Extract the class probabilities for the level "0" (status_binary == 0)
probability_0 <- predictions[, grep("0", colnames(predictions))]
# Preprocess the data by aggregating features for each loan_id
df_aggregated <- df_train %>%
group_by(loan_id) %>%
summarise(
mean_amount_loan = mean(amount_loan),
mean_duration = mean(duration),
mean_payments = mean(payments),
mean_amount_trans = mean(amount_trans),
mean_balance = mean(balance),
mean_date_diff = mean(date_diff),
status_binary = first(status_binary)  # Assuming it's constant for each loan_id
) %>%
ungroup()
# Split the data into training and testing sets
set.seed(42)
train_indices <- sample(1:nrow(df_aggregated), 0.8 * nrow(df_aggregated))
train_data <- df_aggregated[train_indices, ]
test_data <- df_aggregated[-train_indices, ]
# Convert status_binary to a factor with more than two levels
train_data$status_binary <- as.factor(train_data$status_binary)
# Train a random forest model for classification
rf_model <- randomForest(status_binary ~ ., data = train_data)
# Make predictions on the test set
# Extract the class probabilities for the level "0" (status_binary == 0)
probability_0 <- predictions[, grep("0", colnames(predictions))]
# Preprocess the data by aggregating features for each loan_id
df_aggregated <- df_train %>%
group_by(loan_id) %>%
summarise(
mean_amount_loan = mean(amount_loan),
mean_duration = mean(duration),
mean_payments = mean(payments),
mean_amount_trans = mean(amount_trans),
mean_balance = mean(balance),
mean_date_diff = mean(date_diff),
status_binary = first(status_binary)  # Assuming it's constant for each loan_id
) %>%
ungroup()
# Split the data into training and testing sets
set.seed(42)
train_indices <- sample(1:nrow(df_aggregated), 0.8 * nrow(df_aggregated))
train_data <- df_aggregated[train_indices, ]
test_data <- df_aggregated[-train_indices, ]
# Convert status_binary to a factor with more than two levels
train_data$status_binary <- as.factor(train_data$status_binary)
# Train a random forest model for classification
rf_model <- randomForest(status_binary ~ ., data = train_data)
# Make predictions on the test set using the trained random forest model
predictions <- predict(rf_model, newdata = test_data, type = "response")
# Extract the class probabilities for the level "0" (status_binary == 0)
probability_0 <- predictions[, grep("0", colnames(predictions))]
# Preprocess the data by aggregating features for each loan_id
df_aggregated <- df_train %>%
group_by(loan_id) %>%
summarise(
mean_amount_loan = mean(amount_loan),
mean_duration = mean(duration),
mean_payments = mean(payments),
mean_amount_trans = mean(amount_trans),
mean_balance = mean(balance),
mean_date_diff = mean(date_diff),
status_binary = first(status_binary)  # Assuming it's constant for each loan_id
) %>%
ungroup()
# Split the data into training and testing sets
set.seed(42)
train_indices <- sample(1:nrow(df_aggregated), 0.8 * nrow(df_aggregated))
train_data <- df_aggregated[train_indices, ]
test_data <- df_aggregated[-train_indices, ]
# Convert status_binary to a factor with more than two levels
train_data$status_binary <- as.factor(train_data$status_binary)
# Train a random forest model for classification
rf_model <- randomForest(status_binary ~ ., data = train_data)
# Make predictions on the test set using the trained random forest model
predictions <- predict(rf_model, newdata = test_data, type = "response")
# Extract the class probabilities for the level "0" (status_binary == 0)
probability_0 <- predictions[, grep("0", colnames(predictions))]
# Preprocess the data by aggregating features for each loan_id
df_aggregated <- df_train %>%
group_by(loan_id) %>%
summarise(
mean_amount_loan = mean(amount_loan),
mean_duration = mean(duration),
mean_payments = mean(payments),
mean_amount_trans = mean(amount_trans),
mean_balance = mean(balance),
mean_date_diff = mean(date_diff),
status_binary = first(status_binary)  # Assuming it's constant for each loan_id
) %>%
ungroup()
# Split the data into training and testing sets
set.seed(42)
train_indices <- sample(1:nrow(df_aggregated), 0.8 * nrow(df_aggregated))
train_data <- df_aggregated[train_indices, ]
test_data <- df_aggregated[-train_indices, ]
# Convert status_binary to a factor with more than two levels
train_data$status_binary <- as.factor(train_data$status_binary)
# Train a random forest model for classification
rf_model <- randomForest(status_binary ~ ., data = train_data)
# Make predictions on the test set using the trained random forest model
predictions <- predict(rf_model, newdata = test_data, type = "response")
# Extract the class probabilities for the level "0" (status_binary == 0)
probability_0 <- predictions[, grep("0", colnames(predictions))]
print(predictions)
probability_0 <- predictions[, grep("0", colnames(predictions))]
print(colnames(predictions))
df <- read_csv("database.csv")
# Convert 'status' to a factor
df$status <- as.factor(df$status)
# Split the data into training and testing sets
set.seed(42)
train_indices <- sample(1:nrow(df), 0.8 * nrow(df))
train_data <- df[train_indices, ]
test_data <- df[-train_indices, ]
# Train a logistic regression model
logit_model <- glm(status ~ ., data = train_data, family = "binomial")
# Make predictions on the test set
probabilities <- predict(logit_model, newdata = test_data, type = "response")
# Assuming you have a data frame named 'result_df' with columns 'loan_id' and 'Probability'
result_df <- data.frame(loan_id = test_data$loan_id, Probability = probabilities)
# Save the result to a CSV file
write.csv(result_df, "result_file_logit.csv", row.names = FALSE)
df <- read_csv("database.csv")
# Convert 'status' to a factor
df$status <- as.factor(df$status)
# Split the data into training and testing sets
set.seed(42)
train_indices <- sample(1:nrow(df), 0.8 * nrow(df))
train_data <- df[train_indices, ]
test_data <- df[-train_indices, ]
# Train a logistic regression model
logit_model <- glm(status ~ ., data = train_data, family = "binomial")
# Make predictions on the test set
probabilities <- predict(logit_model, newdata = test_data, type = "response")
# Assuming you have a data frame named 'result_df' with columns 'loan_id' and 'Probability'
result_df <- data.frame(loan_id = test_data$loan_id, Probability = probabilities)
# Assuming 'result_df' has columns 'loan_id', 'Probability', and 'status' for the test set
# Merge the results with the actual 'status' from the test set
result_df <- merge(result_df, test_data[c("loan_id", "status")], by = "loan_id")
# Set a threshold to classify probabilities into status -1 or 1
threshold <- 0.5
result_df$predicted_status <- ifelse(result_df$Probability >= threshold, 1, -1)
# Create a confusion matrix
conf_matrix <- table(result_df$predicted_status, result_df$status)
# Calculate accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Accuracy:", accuracy, "\n")
install.packages("xgboost")
library(xgboost)
df <- read_csv("database.csv")
result_split <- split_data(df)
train_data <- result_split$train
test_data <- result_split$test
# Prepare the data
train_matrix <- xgb.DMatrix(data = as.matrix(train_data[, -c("loan_id", "status_binary")]), label = train_data$status_binary)
train_matrix <- xgb.DMatrix(data = as.matrix(train_data[, -c("loan_id", "status")]), label = train_data$status)
print(train_data)
# Create a binary variable indicating whether the status is -1
train_data$status_binary <- as.factor(ifelse(train_data$status == -1, 1, 0))
train_matrix <- xgb.DMatrix(data = as.matrix(train_data[, -c("loan_id", "status_binary")]), label = train_data$status_binary)
train_matrix <- xgb.DMatrix(data = as.matrix(train_data[, !(colnames(train_data) %in% c("loan_id", "status_binary"))]), label = train_data$status_binary)
test_matrix <- xgb.DMatrix(data = as.matrix(test_data[, !(colnames(train_data) %in% c("loan_id", "status_binary"))]), label = test_data$status_binary)
test_matrix <- xgb.DMatrix(data = as.matrix(test_data[, !(colnames(test_data) %in% c("loan_id", "status_binary"))]), label = test_data$status_binary)
df <- read_csv("database.csv")
result_split <- split_data(df)
# Create a binary variable indicating whether the status is -1
df$status_binary <- as.factor(ifelse(df$status == -1, 1, 0))
train_data <- result_split$train
test_data <- result_split$test
df <- read_csv("database.csv")
result_split <- split_data(df)
# Create a binary variable indicating whether the status is -1
df$status_binary <- as.factor(ifelse(df$status == -1, 1, 0))
train_data <- result_split$train
test_data <- result_split$test
print(df)
# Prepare the data
train_matrix <- xgb.DMatrix(data = as.matrix(train_data[, !(colnames(train_data) %in% c("loan_id", "status_binary"))]), label = train_data$status_binary)
df <- read_csv("database.csv")
result_split <- split_data(df)
train_data <- result_split$train
test_data <- result_split$test
# Create a binary variable indicating whether the status is -1
train_data$status_binary <- as.factor(ifelse(train_data$status == -1, 1, 0))
test_data$status_binary <- as.factor(ifelse(test_data$status == -1, 1, 0))
# Prepare the data
train_matrix <- xgb.DMatrix(data = as.matrix(train_data[, !(colnames(train_data) %in% c("loan_id", "status_binary"))]), label = train_data$status_binary)
test_matrix <- xgb.DMatrix(data = as.matrix(test_data[, !(colnames(test_data) %in% c("loan_id", "status_binary"))]), label = test_data$status_binary)
# Set hyperparameters (you may need to tune these)
params <- list(
objective = "binary:logistic",
eval_metric = "logloss",
eta = 0.01,
max_depth = 6,
subsample = 0.8,
colsample_bytree = 0.8
)
xgb_model <- xgboost(params = params, data = train_matrix, nrounds = 1000, early_stopping_rounds = 50, verbose = TRUE)
print(train_data)
df <- read_csv("database.csv")
result_split <- split_data(df)
train_data <- result_split$train
test_data <- result_split$test
# Create a binary variable indicating whether the status is -1
train_data$status_binary <- as.factor(ifelse(train_data$status == -1, 1, 0))
test_data$status_binary <- as.factor(ifelse(test_data$status == -1, 1, 0))
train_data <- train_data[, !colnames(train_data) %in% c("status")]
test_data <- test_data[, !colnames(test_data) %in% c("status")]
print(train_data)
df <- read_csv("database.csv")
result_split <- split_data(df)
train_data <- result_split$train
test_data <- result_split$test
# Create a binary variable indicating whether the status is -1
train_data["status_binary"] <- as.factor(ifelse(train_data$status == -1, 1, 0))
train_data <- train_data[, !colnames(train_data) %in% c("status")]
print(train_data)
